# Sparse-to-Dense Depth Prediction for ADT

This module implements sparse-to-dense depth prediction using ORB-SLAM3's sparse map points as input.

## Overview

The pipeline takes sparse depth maps generated by ORB-SLAM3 and predicts dense depth maps using a U-Net architecture. This is specifically optimized for the Aria Digital Twin (ADT) dataset.

## Dataset Location

ADT sequences are located at: `/mnt/ssd_ext/incSeg-data/adt/`

Available sequences (10 downloaded):
```bash
ls /mnt/ssd_ext/incSeg-data/adt/
# Example: Apartment_release_clean_seq131_M1292/
```

## Complete Step-by-Step Execution

### Step 0: Setup Environment and Build
```bash
# Activate conda environment
source /home/external/miniconda3/bin/activate
conda activate orbslam

# Setup ORB-SLAM3 environment
cd /home/external/ORB_SLAM3_VIO
source setup_env.sh

# Clean and rebuild if needed (only necessary first time or after code changes)
# If you get CMake cache errors, clean the build directories first:
# rm -rf Thirdparty/DBoW2/build Thirdparty/g2o/build Thirdparty/Sophus/build build/
./build.sh
```

### Step 1: Choose an ADT Sequence
```bash
# Option A: Use a specific sequence
VRS_FILE="/mnt/ssd_ext/incSeg-data/adt/Apartment_release_clean_seq131_M1292/ADT_Apartment_release_clean_seq131_M1292_main_recording.vrs"

# Option B: List all available sequences and choose one
ls /mnt/ssd_ext/incSeg-data/adt/*/*main_recording.vrs
```

### Step 2: Convert VRS to TUM-VI Format
```bash
# Convert 30 seconds for testing (remove --duration for full sequence)
python aria_to_tumvi.py "$VRS_FILE" output_tumvi
```

### Step 3: Run ORB-SLAM3 with Tracking Data Save
```bash
# Enable tracking data save
export SAVE_TRACKING=1

# Run SLAM with viewer
./run_orbslam3_aria_tumvi.sh output_tumvi my_trajectory

# Tracking data will be saved to: results/tracking_data_my_trajectory/
```

### Step 4: Process SLAM Output to Sparse Depth
```bash
cd depthSparse2Dense

python process_slam_to_sparse_depth_adt.py \
    ../results/tracking_data_my_trajectory \
    ../output_tumvi \
    sparse_depth_output \
    --visualize

# This creates:
# - sparse_depth_output/sparse_depth/: Sparse depth maps
# - sparse_depth_output/rgb/: Corresponding RGB images
# - sparse_depth_output/poses/: Camera poses
# - sparse_depth_output/metadata/: Camera parameters and frame info
```

### Step 5: Train Sparse-to-Dense Model
```bash
python train_sparse_to_dense.py \
    sparse_depth_output \
    trained_models \
    --epochs 100 \
    --batch_size 8 \
    --learning_rate 1e-4 \
    --model unet

# Monitor training with tensorboard
tensorboard --logdir trained_models/runs
```

### Alternative: Complete Pipeline Script
```bash
cd depthSparse2Dense
./run_adt_pipeline.sh "$VRS_FILE" 30
```

## Working with Multiple Sequences

### Process Multiple ADT Sequences
```bash
# Process all downloaded sequences
for vrs in /mnt/ssd_ext/incSeg-data/adt/*/*main_recording.vrs; do
    seq_name=$(basename $(dirname "$vrs"))
    echo "Processing $seq_name..."
    
    # Convert to TUM-VI
    python ../aria_to_tumvi.py "$vrs" "tumvi_$seq_name" --duration 60
    
    # Run SLAM
    SAVE_TRACKING=1 ../run_orbslam3_aria_tumvi.sh "tumvi_$seq_name" "$seq_name"
    
    # Process to sparse depth
    python process_slam_to_sparse_depth_adt.py \
        "../results/tracking_data_$seq_name" \
        "tumvi_$seq_name" \
        "sparse_$seq_name" \
        --visualize
done
```

### Combine Multiple Sequences for Training
```bash
# Create combined dataset
mkdir -p combined_sparse_data/{sparse_depth,rgb,poses,metadata}

# Copy data from all sequences
for seq_dir in sparse_seq*/; do
    cp "$seq_dir"/sparse_depth/* combined_sparse_data/sparse_depth/
    cp "$seq_dir"/rgb/* combined_sparse_data/rgb/
    cp "$seq_dir"/poses/* combined_sparse_data/poses/
done

# Merge metadata (requires custom script)
python merge_metadata.py sparse_seq*/metadata/frames.json > combined_sparse_data/metadata/frames.json

# Train on combined data
python train_sparse_to_dense.py \
    combined_sparse_data \
    final_model \
    --epochs 200 \
    --batch_size 16
```

## Extracting 3D Map Points (Alternative)

If you want to extract the full 3D map points (not just sparse depth maps), you can follow the original instructions in README_ARIA.md:

```bash
# After running SLAM, extract 3D point cloud
# This saves to processed_pointmap/ directory
# (Make sure System.cc has the map point export functionality)
```

Note: The sparse depth maps created by `process_slam_to_sparse_depth_adt.py` are 2D projections of these 3D points onto the camera frame, which is what we need for training the depth prediction network.

## Example with Specific ADT Sequences

### Example 1: Process seq131
```bash
# Setup
cd /home/external/ORB_SLAM3_VIO
source /home/external/miniconda3/bin/activate && conda activate orbslam
source setup_env.sh

# Define sequence
VRS_FILE="/mnt/ssd_ext/incSeg-data/adt/Apartment_release_clean_seq131_M1292/ADT_Apartment_release_clean_seq131_M1292_main_recording.vrs"

# Step 1: Convert VRS (30 seconds)
python aria_to_tumvi.py "$VRS_FILE" aria_seq131_30s --duration 30

# Step 2: Run SLAM with tracking save
SAVE_TRACKING=1 ./run_orbslam3_aria_tumvi.sh aria_seq131_30s seq131_test

# Step 3: Process to sparse depth
cd depthSparse2Dense
python process_slam_to_sparse_depth_adt.py \
    ../results/tracking_data_seq131_test \
    ../aria_seq131_30s \
    sparse_seq131 \
    --visualize

# Step 4: Train model
python train_sparse_to_dense.py \
    sparse_seq131 \
    model_seq131 \
    --epochs 50 \
    --batch_size 4 \
    --learning_rate 1e-4
```

### Example 2: Quick test with all 10 sequences
```bash
# List all available sequences
ls -d /mnt/ssd_ext/incSeg-data/adt/*/

# Process first 10 seconds of each for quick test
cd /home/external/ORB_SLAM3_VIO/depthSparse2Dense
for vrs in /mnt/ssd_ext/incSeg-data/adt/*/*main_recording.vrs; do
    seq_name=$(basename $(dirname "$vrs") | cut -d'_' -f4)  # Extract seq number
    echo "Quick test on $seq_name (10 seconds)..."
    ./run_adt_pipeline.sh "$vrs" 10
done
```

## Architecture

### U-Net Model (`unet_depth.py`)
- **Input**: 4 channels (RGB + sparse depth)
- **Output**: 1 channel (dense depth)
- **Features**: Skip connections, batch normalization
- **Activation**: ReLU for positive depth values

### Data Pipeline
1. **SLAM Tracking**: Saves 2D feature points with depth from 3D map points
2. **Sparse Depth Maps**: Projects SLAM points to camera frame (~1% density)
3. **Training**: Combines RGB images with sparse depth for dense prediction

## ADT Specific Details

### Camera Parameters (SLAM Camera)
- Resolution: 640×480 (after 90° rotation)
- Focal length: ~242.7 pixels
- Principal point: (318.08, 235.65)

### Data Format
- Sparse depth: Float32 arrays with 0 for missing values
- Confidence: 0-1 based on observation count
- Poses: 4×4 transformation matrices (camera to world)

## Training Options

### Basic Training
```bash
python train_sparse_to_dense.py data_dir output_dir
```

### Advanced Options
```bash
python train_sparse_to_dense.py \
    data_dir \
    output_dir \
    --model unet \              # or 'advanced' for dual-encoder
    --epochs 100 \
    --batch_size 8 \
    --learning_rate 1e-4 \
    --sparse_weight 0.5 \       # Weight for sparse depth loss
    --smooth_weight 0.1 \       # Weight for smoothness loss
    --target_size 480 640       # Resize to specific dimensions
```

## Loss Functions

The training uses a combined loss:
1. **Sparse Depth Loss**: L1 loss at sparse point locations
2. **Smoothness Loss**: Edge-aware smoothness regularization
3. **Optional**: Photometric loss for self-supervision

## Evaluation

After training, evaluate the model:
```bash
python evaluate_sparse_to_dense.py \
    checkpoint.pth \
    test_data_dir \
    --save_predictions
```

## Files Description

- `process_slam_to_sparse_depth_adt.py`: Converts SLAM tracking to sparse depth maps
- `slam_dataloader.py`: PyTorch dataset for loading sparse-dense pairs
- `unet_depth.py`: U-Net model architectures
- `train_sparse_to_dense.py`: Training script
- `losses.py`: Loss functions for depth prediction
- `run_adt_pipeline.sh`: Complete pipeline automation

## Tips for Better Results

1. **More SLAM Features**: Increase ORB features for denser sparse maps
2. **Longer Sequences**: Use more frames for better map coverage
3. **Data Augmentation**: Enable augmentation for better generalization
4. **Learning Rate**: Start with 1e-4, reduce on plateau

## Known Limitations

1. **Sparsity**: SLAM provides <1% of pixels with depth
2. **Scale Ambiguity**: Monocular SLAM has scale drift (IMU helps)
3. **Dynamic Objects**: SLAM assumes static scenes
4. **Tracking Loss**: No sparse depth when SLAM loses tracking

## Future Improvements

1. Integration with ADT ground truth depth for hybrid training
2. Temporal consistency using sequential frames
3. Confidence-weighted loss functions
4. Real-time inference optimization

## Summary of Commands for ADT Dataset

```bash
# 1. List available sequences
cd /home/external/ORB_SLAM3_VIO/depthSparse2Dense
./list_adt_sequences.sh

# 2. Run complete pipeline on one sequence (e.g., seq131)
./run_adt_pipeline.sh /mnt/ssd_ext/incSeg-data/adt/Apartment_release_clean_seq131_M1292/ADT_Apartment_release_clean_seq131_M1292_main_recording.vrs 30

# 3. Or run step by step:
cd /home/external/ORB_SLAM3_VIO
source /home/external/miniconda3/bin/activate && conda activate orbslam && source setup_env.sh

# Build if needed (with clean build for CMake errors)
# rm -rf Thirdparty/DBoW2/build Thirdparty/g2o/build Thirdparty/Sophus/build build/
# ./build.sh

# Convert VRS
python aria_to_tumvi.py /mnt/ssd_ext/incSeg-data/adt/Apartment_release_clean_seq131_M1292/ADT_Apartment_release_clean_seq131_M1292_main_recording.vrs output_seq131 --duration 30

# Run SLAM with tracking save
SAVE_TRACKING=1 ./run_orbslam3_aria_tumvi.sh output_seq131 seq131_slam

# Process to sparse depth
cd depthSparse2Dense
python process_slam_to_sparse_depth_adt.py ../results/tracking_data_seq131_slam ../output_seq131 sparse_seq131 --visualize

# Train model
python train_sparse_to_dense.py sparse_seq131 model_seq131 --epochs 100 --batch_size 8
```

## Output Structure

After processing, you'll have:
```
sparse_seq131/
├── sparse_depth/     # Sparse depth maps from SLAM (NPY files)
├── rgb/              # Corresponding RGB images (PNG files)
├── poses/            # Camera poses (NPY files)
└── metadata/         # Camera parameters and frame info
    ├── intrinsics.npy
    ├── camera_params.json
    └── frames.json

model_seq131/
├── checkpoints/      # Saved model weights
├── runs/            # Tensorboard logs
└── config.json      # Training configuration
```