# Sparse-to-Dense Depth Prediction for ADT

This module implements sparse-to-dense depth prediction using ORB-SLAM3's sparse map points as input, specifically optimized for the Aria Digital Twin (ADT) dataset.

## Overview

The pipeline takes sparse depth maps generated by ORB-SLAM3 and predicts dense depth maps using a U-Net architecture.

**Dataset Location**: `/mnt/ssd_ext/incSeg-data/adt/`
- 8 sequences for training/validation
- 2 sequences for testing

## Prerequisites

```bash
# Activate environment
source /home/external/miniconda3/bin/activate
conda activate orbslam

# Setup ORB-SLAM3
cd /home/external/ORB_SLAM3_VIO
source setup_env.sh
./build.sh
```

## Option 1: Single Sequence Example

Complete step-by-step execution for learning and testing:

### Step 1: Choose an ADT Sequence
```bash
# Example: Use seq131
VRS_FILE="/mnt/ssd_ext/incSeg-data/adt/train/Apartment_release_clean_seq131_M1292/ADT_Apartment_release_clean_seq131_M1292_main_recording.vrs"
```

### Step 2: Convert VRS to TUM-VI Format
```bash
# Convert 30 seconds for testing (remove --duration for full sequence)
python aria_to_tumvi.py "$VRS_FILE" output_tumvi --duration 30
```

### Step 3: Run ORB-SLAM3 with Tracking Data Save
```bash
# Enable tracking data save
export SAVE_TRACKING=1

# Run SLAM with viewer
./run_orbslam3_aria_tumvi.sh output_tumvi my_trajectory

# Tracking data saved to: results/tracking_data_my_trajectory/
```

### Step 4: Process SLAM Output to Sparse Depth
```bash
cd depthSparse2Dense

python process_slam_to_sparse_depth_adt.py \
    ../results/tracking_data_my_trajectory \
    ../output_tumvi \
    sparse_depth_output \
    --visualize
```

### Step 5: Train Sparse-to-Dense Model
```bash
python train_sparse_to_dense.py \
    --data_dir sparse_depth_output \
    --output_dir trained_models \
    --epochs 100 \
    --batch_size 32 \
    --learning_rate 2e-4 \
    --model unet

# Monitor with tensorboard
tensorboard --logdir trained_models/tensorboard
```

## Option 2: Multi-Sequence Pipeline (Recommended)

Process all 8 training sequences automatically:

### Quick Start
```bash
cd depthSparse2Dense

# Test with 2 sequences first (30s each)
./test_multi_sequence_pipeline.sh

# Run full pipeline on all 8 training sequences
./run_multi_sequence_pipeline.sh
```

### What the Pipeline Does

1. **Converts** all 8 VRS files to TUM-VI format
2. **Runs** ORB-SLAM3 on each sequence with tracking save
3. **Generates** sparse depth maps from SLAM output
4. **Merges** all sequences into single training dataset
5. **Trains** model with automatic train/val split (90/10)

### Process Test Sequences

After training completes:
```bash
# Process 2 test sequences
./process_test_sequences.sh

# Evaluate model
python evaluate_on_test.py \
    --model_checkpoint trained_models_multi/checkpoint_best.pth \
    --test_dir test_sequences_processed/sparse_depth
```

## Model Architecture

- **U-Net**: 31M parameters, ~21ms inference on RTX A6000
- **Input**: 4 channels (RGB + sparse depth)
- **Output**: 1 channel (dense depth)
- **Sparsity**: ~0.12% pixels have depth from SLAM

## Training Options

### Multi-GPU Training
```bash
# Automatically detects GPUs and scales batch size
python train_sparse_to_dense.py \
    --data_dir merged_training_data \
    --output_dir trained_models_multi \
    --epochs 100 \
    --batch_size 128  # For 4 GPUs
    --learning_rate 4e-4  # Scaled for larger batch
```

### Resume Training
```bash
python train_sparse_to_dense.py \
    --data_dir merged_training_data \
    --output_dir trained_models_multi \
    --resume
```

## Directory Structure

```
depthSparse2Dense/
├── sparse_depth_output/        # Single sequence output
├── merged_training_data/       # Multi-sequence merged data
├── trained_models_multi/       # Trained model checkpoints
└── test_sequences_processed/   # Processed test data
```

## Performance

- **SLAM**: ~27ms per frame (37 FPS)
- **Sparse-to-Dense**: ~21ms per frame (48 FPS)
- **Combined**: ~48ms per frame (21 FPS)

## Tips

1. **Test first**: Use `test_multi_sequence_pipeline.sh` before full run
2. **Monitor GPU**: Use `nvidia-smi` to check GPU utilization
3. **Check progress**: `tensorboard --logdir trained_models_multi/tensorboard`
4. **Resume on failure**: Training checkpoints are saved automatically

## Time Estimates

- Single sequence (30s): ~1 hour total
- Full pipeline (8 sequences): ~18 hours
- Test processing (2 sequences): ~2 hours

For detailed documentation, see `MULTI_SEQUENCE_README.md`